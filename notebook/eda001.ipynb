{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "- consider event only if there is a significant (score > 0.2) and same type event is near (2-3 hour) it \n",
    "- power averaging\n",
    "\n",
    "- try with 4 folds\n",
    "- try with 6 folds\n",
    "- try with full data\n",
    "\n",
    "- try again gaussian loss\n",
    "\n",
    "What works and gives boost but overfits:\n",
    "- LSTM encoder (0.772/~0.722)\n",
    "- resnet50 (0.780/0.722)\n",
    "- duration 17280 (0.772/0.722)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1662410/251296522.py:39: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=config_path.as_posix())\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from src.utils.metrics import event_detection_ap\n",
    "\n",
    "from src.utils.post_process import post_process_for_seg\n",
    "import jupyter_black\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "\n",
    "jupyter_black.load()\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "INFERENCE = False\n",
    "EXP_NAME = \"transformer_dur_8640_folds\"\n",
    "RUN_NAME = \"run0\"\n",
    "TYPE = \"score\"\n",
    "\n",
    "if INFERENCE:\n",
    "    RESULT_DIR = Path(\"../output/inference\") / EXP_NAME  # / RUN_NAME\n",
    "    hydra_result_dir = Path(\"../output/train\") / \"transformer_dur_8640_folds\" / RUN_NAME\n",
    "else:\n",
    "    RESULT_DIR = Path(\"../output/train\") / EXP_NAME / RUN_NAME\n",
    "    hydra_result_dir = Path(\"../output/train\") / EXP_NAME / RUN_NAME\n",
    "\n",
    "\n",
    "def load_config(result_dir: Path):\n",
    "    # clear previous initialization\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "    # initialize hydra\n",
    "    config_path = result_dir / \".hydra\"\n",
    "    initialize(config_path=config_path.as_posix())\n",
    "    # load the config\n",
    "    cfg = compose(config_name=\"config\")\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg = load_config(hydra_result_dir)\n",
    "\n",
    "if INFERENCE:\n",
    "    preds = np.load(RESULT_DIR / f\"preds.npy\")\n",
    "    keys = np.load(RESULT_DIR / f\"keys.npy\")\n",
    "else:\n",
    "    preds = np.load(RESULT_DIR / f\"preds_{TYPE}.npy\")\n",
    "    labels = np.load(RESULT_DIR / f\"labels_{TYPE}.npy\")\n",
    "    keys = np.load(RESULT_DIR / \"keys.npy\")\n",
    "\n",
    "gt_df = pd.read_csv(Path(cfg.dir.data_dir) / \"train_events.csv\")\n",
    "gt_df = gt_df[gt_df[\"series_id\"].isin(cfg.split.valid_series_ids)].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize postprocess parameters using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7672724347834871, 54999)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(Path(cfg.dir.processed_dir) / \"train\" / \"series_lens.json\") as f:\n",
    "    series_lens = json.load(f)\n",
    "\n",
    "pred_df = post_process_for_seg(keys, preds, series_lens, score_th=0.0001, distance=70, offset=8)\n",
    "# pred_df = pred_df.to_pandas()\n",
    "\n",
    "score = event_detection_ap(gt_df, pred_df)\n",
    "\n",
    "score, len(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 5_0 - 0.765 / 0.733 - rank 1\n",
    "# fold 5_3 - 0.772 / 0.733 - rank 3\n",
    "\n",
    "# fold 10_2 - 0.0001 / 85 / 0.811 / 0.729 (0.709 for the best loss) - rank 6\n",
    "# fold 10_4 - 0.0001 / 70 / 0.789 / 0.734 - rank 2\n",
    "# fold 10_6 - 0.0001 / 70 / 0.779 / 0.733 - rank 4\n",
    "# fold 10_7 - 0.0001 / 80 / 0.792 / 0.729 - rank 5\n",
    "# fold 10_9 - 0.0001 / 80 / 0.812 / 0.722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    # score_th = 0.005\n",
    "    score_th = trial.suggest_float(\"score_th\", 0, 0.5)\n",
    "    distance = trial.suggest_int(\"distance\", 30, 400)\n",
    "\n",
    "    pred_df: pl.DataFrame = post_process_for_seg(keys, preds, score_th=score_th, distance=distance)\n",
    "    score = event_detection_ap(gt_df, pred_df)\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_sample(gt_df, keys, preds, labels, num_samples=1, num_chunks=10):\n",
    "    # get series ids\n",
    "    series_ids = np.array(list(map(lambda x: x.split(\"_\")[0], keys)))\n",
    "    unique_series_ids = np.unique(series_ids)\n",
    "\n",
    "    # get random series\n",
    "    random_series_ids = np.random.choice(unique_series_ids, num_samples)\n",
    "\n",
    "    for i, random_series_id in enumerate(random_series_ids):\n",
    "        # get random series\n",
    "        series_idx = np.where(series_ids == random_series_id)[0]\n",
    "        this_series_preds = preds[series_idx].reshape(-1, 3)\n",
    "        this_series_labels = labels[series_idx].reshape(-1, 3)\n",
    "\n",
    "        # split series\n",
    "        this_series_preds = np.split(this_series_preds, num_chunks)\n",
    "        this_series_labels = np.split(this_series_labels, num_chunks)\n",
    "        this_series_len = [0] + [len(x) for x in this_series_labels]\n",
    "        this_series_len = np.cumsum(this_series_len)\n",
    "\n",
    "        gt_df = gt_df[gt_df[\"series_id\"] == random_series_id]\n",
    "\n",
    "        fig, axs = plt.subplots(num_chunks, 1, figsize=(20, 5 * num_chunks))\n",
    "\n",
    "        if num_chunks == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for j in range(num_chunks):\n",
    "            this_series_preds_chunk = this_series_preds[j]\n",
    "            this_series_labels_chunk = this_series_labels[j]\n",
    "\n",
    "            # get onset and wakeup idx\n",
    "            gt_tmp = gt_df[\n",
    "                (gt_df[\"step\"] >= this_series_len[j]) & (gt_df[\"step\"] <= this_series_len[j + 1])\n",
    "            ]\n",
    "            onset_idx = gt_tmp.loc[gt_tmp[\"event\"] == \"onset\", \"step\"].to_list()\n",
    "            onset_idx = onset_idx - this_series_len[j]\n",
    "            wakeup_idx = gt_tmp.loc[gt_tmp[\"event\"] == \"wakeup\", \"step\"].to_list()\n",
    "            wakeup_idx = wakeup_idx - this_series_len[j]\n",
    "\n",
    "            axs[j].plot(this_series_preds_chunk[:, 0], label=\"pred_sleep\")\n",
    "            axs[j].plot(this_series_preds_chunk[:, 1], label=\"pred_onset\")\n",
    "            axs[j].plot(this_series_preds_chunk[:, 2], label=\"pred_wakeup\")\n",
    "            axs[j].vlines(onset_idx, 0, 1, label=\"onset\", linestyles=\"dashed\", color=\"C1\")\n",
    "            axs[j].vlines(wakeup_idx, 0, 1, label=\"wakeup\", linestyles=\"dashed\", color=\"C2\")\n",
    "            axs[j].set_ylim(0, 1)\n",
    "            axs[j].set_title(f\"series_id: {random_series_id} chunk_id: {j}\")\n",
    "            axs[j].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_random_sample(gt_df, keys, preds, labels, num_chunks=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "BEST_MODEL = \"ensemble\"\n",
    "FOLD = 0\n",
    "\n",
    "DURATION = 8640\n",
    "DOWNSAMPLE_RATE = 2\n",
    "PHASE = \"train\"\n",
    "EXP_NAME = \"transformer_best_folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[5.0GB(+3.9GB):2.0sec] load test dataloader \n",
      "List of models: ['/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_0.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_2.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_4.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_6.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_7.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_3.pth']\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_0.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_2.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_4.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_6.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_7.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_3.pth\n",
      "[6.0GB(+1.0GB):2.7sec] load model \n",
      "inference: 100%|████████████████████████████| 1867/1867 [01:23<00:00, 22.44it/s]\n",
      "[7.6GB(+1.7GB):97.8sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [01:24<00:00, 22.15it/s]\n",
      "[7.7GB(+0.0GB):100.7sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [01:21<00:00, 22.92it/s]\n",
      "[7.7GB(+0.0GB):97.8sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [01:21<00:00, 22.96it/s]\n",
      "[7.7GB(+0.0GB):97.4sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [01:16<00:00, 24.52it/s]\n",
      "[7.7GB(+0.0GB):93.2sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [01:16<00:00, 24.28it/s]\n",
      "[7.7GB(+0.0GB):92.4sec] inference \n",
      "[7.7GB(+0.0GB):1.8sec] make submission \n"
     ]
    }
   ],
   "source": [
    "!python -m run.inference\\\n",
    "    dir=local\\\n",
    "    model.params.encoder_name=resnet34\\\n",
    "    model.params.encoder_weights=null\\\n",
    "    num_workers=12\\\n",
    "    exp_name=$EXP_NAME\\\n",
    "    weight.run_name=single\\\n",
    "    batch_size=8\\\n",
    "    duration=$DURATION\\\n",
    "    downsample_rate=$DOWNSAMPLE_RATE\\\n",
    "    pp.score_th=0.0015\\\n",
    "    pp.distance=70\\\n",
    "    phase=$PHASE\\\n",
    "    best_model=$BEST_MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
