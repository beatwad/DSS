{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n",
      "617\n",
      "538\n",
      "713\n"
     ]
    }
   ],
   "source": [
    "# 1, 3, 5, 6\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "strange_series = [\n",
    "    \"05e1944c3818\",  # 1\n",
    "    \"13b4d6a01d27\",  # 4\n",
    "    \"7476c0bd18d2\",  # 3\n",
    "    \"5aad18e7ce64\",  # 2\n",
    "    \"aed3850f65f0\",  # 5\n",
    "    \"c5365a55ebb7\",  # 2\n",
    "    \"f981a0805fd0\",  # 4\n",
    "    \"0f9e60a8e56d\",  # 3\n",
    "    \"2fc653ca75c7\",  # 5\n",
    "    \"390b487231ce\",  # 4\n",
    "    \"89c7daa72eee\",  # 1\n",
    "    \"a3e59c2ce3f6\",  # 5\n",
    "    \"c5d08fc3e040\",  # 3\n",
    "    \"c7b1283bb7eb\",  # 2\n",
    "    \"e11b9d69f856\",  # 1\n",
    "]\n",
    "\n",
    "x = pd.read_csv(\"../data/child-mind-institute-detect-sleep-states/train_events.csv\")\n",
    "x[\"is_strange\"] = 0\n",
    "x[\"fold\"] = 0\n",
    "x.loc[x[\"series_id\"].isin(strange_series), \"is_strange\"] = 1\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=4)\n",
    "\n",
    "x = x[x[\"event\"] == \"onset\"].reset_index(drop=True)\n",
    "X = x[[\"night\"]]\n",
    "y = x[\"is_strange\"]\n",
    "groups = x[\"series_id\"]\n",
    "\n",
    "res = \"\"\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sgkf.split(X, y, groups)):\n",
    "    series = x.loc[test_index, \"series_id\"].unique()\n",
    "    x.loc[test_index, \"fold\"] = i\n",
    "    print(x.loc[test_index, \"step\"].isna().sum())\n",
    "\n",
    "    for s in series:\n",
    "        res += f\"- {s}\\n\"\n",
    "    res += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 04f547b8017d\n",
      "- 0a96f4993bd7\n",
      "- 0cd1e3d0ed95\n",
      "- 0d0ad1e77851\n",
      "- 0f9e60a8e56d\n",
      "- 12d01911d509\n",
      "- 1762ab70ec76\n",
      "- 188d4b7cd28b\n",
      "- 1c7c0bad1263\n",
      "- 280e08693c6d\n",
      "- 292a75c0b94e\n",
      "- 29c75c018220\n",
      "- 29d3469bd15d\n",
      "- 2fbbee1a38e3\n",
      "- 349c5562ee2c\n",
      "- 3df0da2e5966\n",
      "- 4a31811f3558\n",
      "- 4ab54be1a403\n",
      "- 4ac356361be9\n",
      "- 4feda0596965\n",
      "- 55b7f5c99930\n",
      "- 599ca4ed791b\n",
      "- 5c088d7e916c\n",
      "- 5f76965e10cf\n",
      "- 5ffd5e1e81ac\n",
      "- 6bf95a3cf91c\n",
      "- 6ee4ade1f2bd\n",
      "- 72bbd1ac3edf\n",
      "- 7476c0bd18d2\n",
      "- 76237b9406d5\n",
      "- 78569a801a38\n",
      "- 808652a666c6\n",
      "- 844f54dcab89\n",
      "- 87a6cbb7c4ed\n",
      "- 8a306e0890c0\n",
      "- 9277be28a1cf\n",
      "- 99b829cbad2d\n",
      "- 9aed9ee12ae2\n",
      "- 9ddd40f2cb36\n",
      "- 9ee455e4770d\n",
      "- a2b0a64ec9cf\n",
      "- ad425f3ee76d\n",
      "- b1831c4979da\n",
      "- b4b75225b224\n",
      "- b750c8c1556c\n",
      "- bb5612895813\n",
      "- bf00506437aa\n",
      "- c289c8a823e0\n",
      "- c38707ef76df\n",
      "- c5365a55ebb7\n",
      "- c68260cc9e8f\n",
      "- c7b1283bb7eb\n",
      "- c7d693f24684\n",
      "- cca14d1966c1\n",
      "- d150801f3145\n",
      "- d2fef7e4defd\n",
      "- dacc6d652e35\n",
      "- dfc3ccebfdc9\n",
      "- dff367373725\n",
      "- e1f2a4f991cb\n",
      "- e30cb792a2bc\n",
      "- e69aff66e0cb\n",
      "- e8d0a37c3eba\n",
      "- ebb6fae8ed43\n",
      "- ebd76e93ec7d\n",
      "- f2c2436cf7b7\n",
      "- f7eb179216c2\n",
      "- fbf33b1a2c10\n",
      "- fcca183903b7\n",
      "\n",
      "- 03d92c9f6f8a\n",
      "- 0402a003dae9\n",
      "- 062cae666e2a\n",
      "- 08db4255286f\n",
      "- 137771d19ca2\n",
      "- 16fe2798ed0f\n",
      "- 1716cd4163b2\n",
      "- 18b61dd5aae8\n",
      "- 1955d568d987\n",
      "- 1d4569cbac0f\n",
      "- 1f96b9668bdf\n",
      "- 207eded97727\n",
      "- 2654a87be968\n",
      "- 31011ade7c0a\n",
      "- 3452b878e596\n",
      "- 3664fe9233f9\n",
      "- 3aceb17ef7bd\n",
      "- 3d53bfea61d6\n",
      "- 405df1b41f9f\n",
      "- 416354edd92a\n",
      "- 4743bdde25df\n",
      "- 483d6545417f\n",
      "- 55a47ff9dc8a\n",
      "- 5aad18e7ce64\n",
      "- 5acc9d63b5fd\n",
      "- 5f40907ec171\n",
      "- 60d31b0bec3b\n",
      "- 612aa8ba44e2\n",
      "- 6ca4f4fca6a2\n",
      "- 703b5efa9bc1\n",
      "- 72ba4a8afff4\n",
      "- 7504165f497d\n",
      "- 7fd4284b7ee8\n",
      "- 804594bb1f06\n",
      "- 83fa182bec3a\n",
      "- 8898e6db816d\n",
      "- 8fb18e36697d\n",
      "- 91127c2b0e60\n",
      "- 99237ce045e4\n",
      "- 9a340507e36a\n",
      "- 9c91c546e095\n",
      "- a261bc4b7470\n",
      "- a9a2f7fac455\n",
      "- af91d9a50547\n",
      "- b364205aba43\n",
      "- b7fc34995d0f\n",
      "- ba8083a2c3b8\n",
      "- bdfce9ce62b9\n",
      "- c535634d7dcd\n",
      "- c5d08fc3e040\n",
      "- c6788e579967\n",
      "- ca730dbf521d\n",
      "- ccdee561ee5d\n",
      "- ce9164297046\n",
      "- d043c0ca71cd\n",
      "- d0f613c700f7\n",
      "- d2d6b9af0553\n",
      "- d93b0c7de16b\n",
      "- db5e0ee1c0ab\n",
      "- db75092f0530\n",
      "- def21f50dd3c\n",
      "- e6ddbaaf0639\n",
      "- ea0770830757\n",
      "- ece2561f07e9\n",
      "- f564985ab692\n",
      "- f6d2cc003183\n",
      "- f981a0805fd0\n",
      "- fa149c3c4bde\n",
      "\n",
      "- 05e1944c3818\n",
      "- 062dbd4c95e6\n",
      "- 0ce74d6d2106\n",
      "- 0dee4fda51c3\n",
      "- 10469f6765bf\n",
      "- 10f8bc1f7b07\n",
      "- 137b99e936ab\n",
      "- 154fe824ed87\n",
      "- 18a0ca03431d\n",
      "- 1e6717d93c1d\n",
      "- 25e2b3dd9c3b\n",
      "- 27f09a6a858f\n",
      "- 2b0a1fa8eba8\n",
      "- 2b8d87addea9\n",
      "- 33ceeba8918a\n",
      "- 361366da569e\n",
      "- 3665c86afaf5\n",
      "- 3a9a9dc2cbd9\n",
      "- 3c336d6ba566\n",
      "- 449766346eb1\n",
      "- 519ae2d858b0\n",
      "- 51b23d177971\n",
      "- 51fdcc8d9fe7\n",
      "- 559ffb7c166a\n",
      "- 5f94bb3e1bed\n",
      "- 67f5fc60e494\n",
      "- 6a4cd123bd69\n",
      "- 6d6b9d22d48a\n",
      "- 702bb5387b1e\n",
      "- 72d2234e84e4\n",
      "- 7822ee8fe3ec\n",
      "- 7df249527c63\n",
      "- 89bd631d1769\n",
      "- 89c7daa72eee\n",
      "- 8a22387617c3\n",
      "- 8b159a98f485\n",
      "- 8e32047cbc1f\n",
      "- 91cb6c98201f\n",
      "- 9b9cd7b7af8c\n",
      "- a167532acca2\n",
      "- a3e59c2ce3f6\n",
      "- a596ad0b82aa\n",
      "- a81f4472c637\n",
      "- a88088855de5\n",
      "- aa81faa78747\n",
      "- aed3850f65f0\n",
      "- b7188813d58a\n",
      "- b737f8c78ec5\n",
      "- bccf2f2819f8\n",
      "- bfa54bd26187\n",
      "- c3072a759efb\n",
      "- c7b2155a4a47\n",
      "- c908a0ad3e31\n",
      "- ca732a3c37f7\n",
      "- cf13ed7e457a\n",
      "- d25e479ecbb7\n",
      "- d3dddd3c0e00\n",
      "- d5e47b94477e\n",
      "- dc80ca623d71\n",
      "- e0686434d029\n",
      "- e1f5abb82285\n",
      "- e2b60820c325\n",
      "- e34b496b84ce\n",
      "- e4500e7e19e1\n",
      "- ee4e0e3afd3d\n",
      "- eef041dd50aa\n",
      "- f88e18cb4100\n",
      "- f8a8da8bdd00\n",
      "- fb223ed2278c\n",
      "- fe90110788d2\n",
      "\n",
      "- 038441c925bb\n",
      "- 0cfc06c129cc\n",
      "- 0ec9fc461819\n",
      "- 0ef7d94fde99\n",
      "- 0f572d690310\n",
      "- 1087d7b0ff2e\n",
      "- 1319a1935f48\n",
      "- 13b4d6a01d27\n",
      "- 148471991ffb\n",
      "- 1b92be89db4c\n",
      "- 2cd2340ca14d\n",
      "- 2e9ced2c7976\n",
      "- 2f7504d0f426\n",
      "- 2fc653ca75c7\n",
      "- 3318a0e3ed6f\n",
      "- 35826366dfc7\n",
      "- 390b487231ce\n",
      "- 3be1545083b7\n",
      "- 3be2f86c3e45\n",
      "- 40dce6018935\n",
      "- 44a41bba1ee7\n",
      "- 44d8c02b369e\n",
      "- 4b45c36f8f5a\n",
      "- 51c49c540b4e\n",
      "- 5c55a5e717d6\n",
      "- 5e816f11f5c3\n",
      "- 601559e1777d\n",
      "- 60e51cad2ffb\n",
      "- 653622ac8363\n",
      "- 655f19eabf1e\n",
      "- 694faf956ebf\n",
      "- 73fb772e50fb\n",
      "- 752900afe3a6\n",
      "- 77ca4db83644\n",
      "- 785c9ca4eff7\n",
      "- 854206f602d0\n",
      "- 8877a6586606\n",
      "- 8b8b9e29171c\n",
      "- 8becc76ea607\n",
      "- 8f6f15b9f598\n",
      "- 90eac42a9ec9\n",
      "- 927dd0c35dfd\n",
      "- 939932f1822d\n",
      "- 971207c6a525\n",
      "- 9fbdeffbe2ba\n",
      "- a4e48102f402\n",
      "- a681f9b04b21\n",
      "- a9e5f5314bcb\n",
      "- b84960841a75\n",
      "- bfe41e96d12f\n",
      "- c107b5789660\n",
      "- c75b4b207bea\n",
      "- c8053490cec2\n",
      "- ce85771a714c\n",
      "- cfeb11428dd7\n",
      "- d515236bdeec\n",
      "- d5be621fd9aa\n",
      "- d8de352c2657\n",
      "- d9e887091a5c\n",
      "- de6fedfb6139\n",
      "- df33ae359fb5\n",
      "- e0d7b0dcf9f3\n",
      "- e11b9d69f856\n",
      "- e2a849d283c0\n",
      "- e586cbfa7762\n",
      "- e867b5133665\n",
      "- eec197a4bdca\n",
      "- efbfc4526d58\n",
      "- f0482490923c\n",
      "- f56824b503a0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "- fix DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632110/3043315833.py:41: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=config_path.as_posix())\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from src.utils.metrics import event_detection_ap\n",
    "\n",
    "from src.utils.post_process import post_process_for_seg\n",
    "import jupyter_black\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "\n",
    "jupyter_black.load()\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "INFERENCE = False\n",
    "EXP_NAME = \"transformer_folds_7_lstm\"\n",
    "RUN_NAME = \"run0\"\n",
    "TYPE = \"score\"\n",
    "\n",
    "if INFERENCE:\n",
    "    RESULT_DIR = Path(\"../output/inference\") / EXP_NAME  # / RUN_NAME\n",
    "    hydra_result_dir = Path(\"../output/train\") / \"transformer_folds_5\" / RUN_NAME\n",
    "else:\n",
    "    RESULT_DIR = Path(\"../output/train\") / EXP_NAME\n",
    "    hydra_result_dir = Path(\"../output/train\") / EXP_NAME / RUN_NAME\n",
    "\n",
    "\n",
    "def load_config(result_dir: Path):\n",
    "    # clear previous initialization\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "    # initialize hydra\n",
    "    config_path = result_dir / \".hydra\"\n",
    "    initialize(config_path=config_path.as_posix())\n",
    "    # load the config\n",
    "    cfg = compose(config_name=\"config\")\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg = load_config(hydra_result_dir)\n",
    "\n",
    "if INFERENCE:\n",
    "    preds = np.load(RESULT_DIR / f\"preds.npy\")\n",
    "    keys = np.load(RESULT_DIR / f\"keys.npy\")\n",
    "else:\n",
    "    preds, labels, keys = None, None, None\n",
    "    for i in range(7):\n",
    "        tmp_preds = np.load(RESULT_DIR / f\"run{i}\" / f\"preds_{TYPE}.npy\")\n",
    "        tmp_labels = np.load(RESULT_DIR / f\"run{i}\" / f\"labels_{TYPE}.npy\")\n",
    "        tmp_keys = np.load(RESULT_DIR / f\"run{i}\" / \"keys.npy\")\n",
    "\n",
    "        if tmp_preds.shape[2] == 3:\n",
    "            tmp_preds = tmp_preds[:, :, [1, 2]]\n",
    "\n",
    "        if tmp_labels.shape[2] == 3:\n",
    "            tmp_labels = tmp_labels[:, :, [1, 2]]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = tmp_preds\n",
    "            labels = tmp_labels\n",
    "            keys = tmp_keys\n",
    "        else:\n",
    "            preds = np.concatenate((preds, tmp_preds))\n",
    "            labels = np.concatenate((labels, tmp_labels))\n",
    "            keys = np.concatenate((keys, tmp_keys))\n",
    "\n",
    "gt_df = pd.read_csv(Path(cfg.dir.data_dir) / \"train_events.csv\").dropna().reset_index(drop=True)\n",
    "# gt_df = gt_df[gt_df[\"series_id\"].isin(cfg.split.valid_series_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize postprocess parameters using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6707851650592838, 470660)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_weak_events(sub_df, event_threshold):\n",
    "    # delete onset and wakeup events that are too far from the same events with high score\n",
    "    sub_df[\"prev_onset_step\"] = np.nan\n",
    "    sub_df[\"next_onset_step\"] = np.nan\n",
    "    sub_df[\"prev_wakeup_step\"] = np.nan\n",
    "    sub_df[\"next_wakeup_step\"] = np.nan\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask = (sub_df[\"event\"] == \"onset\") & (sub_df[\"score\"] >= 0.1)\n",
    "    sub_df.loc[mask, \"prev_high_score_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_high_score_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_high_score_onset_step\"] = sub_df[\"prev_high_score_onset_step\"].ffill()\n",
    "    sub_df[\"next_high_score_onset_step\"] = sub_df[\"next_high_score_onset_step\"].bfill()\n",
    "\n",
    "    mask = (\n",
    "        (sub_df[\"event\"] == \"onset\")\n",
    "        & (sub_df[\"step\"] - sub_df[\"prev_high_score_onset_step\"] >= event_threshold)\n",
    "        & (sub_df[\"next_high_score_onset_step\"] - sub_df[\"step\"] >= event_threshold)\n",
    "    )\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask = (sub_df[\"event\"] == \"wakeup\") & (sub_df[\"score\"] >= 0.1)\n",
    "    sub_df.loc[mask, \"prev_high_score_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_high_score_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_high_score_wakeup_step\"] = sub_df[\"prev_high_score_wakeup_step\"].ffill()\n",
    "    sub_df[\"next_high_score_wakeup_step\"] = sub_df[\"next_high_score_wakeup_step\"].bfill()\n",
    "\n",
    "    mask = (\n",
    "        (sub_df[\"event\"] == \"wakeup\")\n",
    "        & (sub_df[\"step\"] - sub_df[\"prev_high_score_wakeup_step\"] >= event_threshold)\n",
    "        & (sub_df[\"next_high_score_wakeup_step\"] - sub_df[\"step\"] >= event_threshold)\n",
    "    )\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def clean_too_far_events(sub_df, event_threshold):\n",
    "    # delete onset and wakeup events that are too far from next wakeup / previous onset event\n",
    "    sub_df[\"prev_onset_step\"] = np.nan\n",
    "    sub_df[\"next_onset_step\"] = np.nan\n",
    "    sub_df[\"prev_wakeup_step\"] = np.nan\n",
    "    sub_df[\"next_wakeup_step\"] = np.nan\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask = sub_df[\"event\"] == \"onset\"\n",
    "    sub_df.loc[mask, \"prev_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_onset_step\"] = sub_df[\"prev_onset_step\"].ffill()\n",
    "    sub_df[\"next_onset_step\"] = sub_df[\"next_onset_step\"].bfill()\n",
    "\n",
    "    mask = sub_df[\"step\"] - sub_df[\"prev_onset_step\"] >= event_threshold\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask = sub_df[\"event\"] == \"wakeup\"\n",
    "    sub_df.loc[mask, \"prev_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_wakeup_step\"] = sub_df[\"prev_wakeup_step\"].ffill()\n",
    "    sub_df[\"next_wakeup_step\"] = sub_df[\"next_wakeup_step\"].bfill()\n",
    "\n",
    "    mask = sub_df[\"next_wakeup_step\"] - sub_df[\"step\"] >= event_threshold\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def delete_alone_events(sub_df, alone_threshold):\n",
    "    # detect alone onset event that are between two other onset events and delete them\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask_1 = (\n",
    "        sub_df[\"step\"] - sub_df[\"prev_onset_step\"].shift(1)\n",
    "        < sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"]\n",
    "    )\n",
    "    mask_2 = sub_df[\"step\"] - sub_df[\"prev_onset_step\"].shift(1) >= alone_threshold\n",
    "    mask_3 = (\n",
    "        sub_df[\"next_onset_step\"].shift(-1) - sub_df[\"step\"]\n",
    "        < sub_df[\"next_wakeup_step\"] - sub_df[\"step\"]\n",
    "    )\n",
    "    mask_4 = sub_df[\"next_onset_step\"].shift(-1) - sub_df[\"step\"] >= alone_threshold\n",
    "    mask_5 = sub_df[\"event\"] == \"onset\"\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask_1 = (\n",
    "        sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"].shift(1)\n",
    "        < sub_df[\"step\"] - sub_df[\"prev_onset_step\"]\n",
    "    )\n",
    "    mask_2 = sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"].shift(1) >= alone_threshold\n",
    "    mask_3 = (\n",
    "        sub_df[\"next_wakeup_step\"].shift(-1) - sub_df[\"step\"]\n",
    "        < sub_df[\"next_onset_step\"] - sub_df[\"step\"]\n",
    "    )\n",
    "    mask_4 = sub_df[\"next_wakeup_step\"].shift(-1) - sub_df[\"step\"] >= alone_threshold\n",
    "    mask_5 = sub_df[\"event\"] == \"wakeup\"\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def delete_events_among_differnet_evnets(sub_df, close_threshold):\n",
    "    # delete events that are among many different events\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask_1 = (\n",
    "        (sub_df[\"event\"].shift(1) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(2) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(3) == \"wakeup\")\n",
    "    )\n",
    "    mask_2 = (\n",
    "        (sub_df[\"event\"].shift(-1) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(-2) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(-3) == \"wakeup\")\n",
    "    )\n",
    "    mask_3 = sub_df[\"event\"] == \"onset\"\n",
    "    mask_4 = sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"] <= close_threshold\n",
    "    mask_5 = sub_df[\"next_wakeup_step\"] - sub_df[\"step\"] <= close_threshold\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask_1 = (\n",
    "        (sub_df[\"event\"].shift(1) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(2) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(3) == \"onset\")\n",
    "    )\n",
    "    mask_2 = (\n",
    "        (sub_df[\"event\"].shift(-1) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(-2) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(-3) == \"onset\")\n",
    "    )\n",
    "    mask_3 = sub_df[\"event\"] == \"wakeup\"\n",
    "    mask_4 = sub_df[\"step\"] - sub_df[\"prev_onset_step\"] <= 360\n",
    "    mask_5 = sub_df[\"next_onset_step\"] - sub_df[\"step\"] <= 360\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def post_process_for_seg(\n",
    "    keys: list[str],\n",
    "    preds: np.ndarray,\n",
    "    series_lens: dict,\n",
    "    score_th: float = 0.01,\n",
    "    distance: int = 5000,\n",
    "    offset: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"make submission dataframe for segmentation task\n",
    "\n",
    "    Args:\n",
    "        keys (list[str]): list of keys. key is \"{series_id}_{chunk_id}\"\n",
    "        preds (np.ndarray): (num_series * num_chunks, duration, 2)\n",
    "        score_th (float, optional): threshold for score. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: submission dataframe\n",
    "    \"\"\"\n",
    "    series_ids = np.array(list(map(lambda x: x.split(\"_\")[0], keys)))\n",
    "    unique_series_ids = np.unique(series_ids)\n",
    "\n",
    "    records = []\n",
    "    for series_id in unique_series_ids:\n",
    "        max_step = series_lens[series_id]\n",
    "\n",
    "        series_idx = np.where(series_ids == series_id)[0]\n",
    "        this_series_preds = preds[series_idx].reshape(-1, 2)\n",
    "\n",
    "        onset_event_preds = this_series_preds[:, 0]\n",
    "        onset_steps = find_peaks(onset_event_preds, height=score_th, distance=distance)[0]\n",
    "        onset_scores = onset_event_preds[onset_steps]\n",
    "        min_onset_step = min(onset_steps) if len(onset_steps) > 0 else 0\n",
    "\n",
    "        wakeup_event_preds = this_series_preds[:, 1]\n",
    "        wakeup_steps = find_peaks(wakeup_event_preds, height=score_th, distance=distance)[0]\n",
    "        wakeup_scores = wakeup_event_preds[wakeup_steps]\n",
    "        max_wakeup_step = max(wakeup_steps) if len(wakeup_steps) > 0 else 0\n",
    "\n",
    "        for step, score in zip(onset_steps, onset_scores):\n",
    "            # select only wakeups than has at least one onset before\n",
    "            # and not too close to series borders\n",
    "            if step >= max_wakeup_step or step <= 720 or step >= max_step - 720 * offset:\n",
    "                continue\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"series_id\": series_id,\n",
    "                    \"step\": step,\n",
    "                    \"event\": \"onset\",\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        for step, score in zip(wakeup_steps, wakeup_scores):\n",
    "            # select only onsets than has at least one wakeup after\n",
    "            # and not too close to series borders\n",
    "            if step <= min_onset_step or step <= 720 * offset or step >= max_step - 180:\n",
    "                continue\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"series_id\": series_id,\n",
    "                    \"step\": step,\n",
    "                    \"event\": \"wakeup\",\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if len(records) == 0:  # If there is no prediction, insert dummy\n",
    "        records.append(\n",
    "            {\n",
    "                \"series_id\": series_id,\n",
    "                \"step\": 0,\n",
    "                \"event\": \"onset\",\n",
    "                \"score\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    sub_df = pd.DataFrame(records).sort_values([\"series_id\", \"step\"]).reset_index(drop=True)\n",
    "    sub_df[\"row_id\"] = np.arange(len(sub_df))\n",
    "\n",
    "    # # delete onset and wakeup events that are too far from the same events with high score\n",
    "    # high_score_event_threshold = 17280 * 1.01  # 24 hours\n",
    "    # sub_df = clean_weak_events(sub_df, high_score_event_threshold)\n",
    "\n",
    "    # # delete onset and wakeup events that are too far from next wakeup / previous onset event\n",
    "    # event_threshold = 14400  # 20 hours\n",
    "    # sub_df = clean_too_far_events(sub_df, event_threshold)\n",
    "\n",
    "    # # detect alone onset event that are between two other onset events and delete them\n",
    "    # alone_threshold = 2880  # 2 hours\n",
    "    # sub_df = delete_alone_events(sub_df, alone_threshold)\n",
    "\n",
    "    # # delete events that are among many different events\n",
    "    # close_threshold = 360  # 30 minutes\n",
    "    # sub_df = delete_events_among_differnet_evnets(sub_df, close_threshold)\n",
    "\n",
    "    sub_df = sub_df[[\"row_id\", \"series_id\", \"step\", \"event\", \"score\"]]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "with open(Path(cfg.dir.processed_dir) / \"train\" / \"series_lens.json\") as f:\n",
    "    series_lens = json.load(f)\n",
    "\n",
    "pred_df = post_process_for_seg(keys, preds, series_lens, score_th=0.0001, distance=70, offset=8)\n",
    "# pred_df = pred_df.to_pandas()\n",
    "\n",
    "score = event_detection_ap(gt_df, pred_df)\n",
    "\n",
    "score, len(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.7675050274210242, 468368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):\n",
    "#     # score_th = 0.005\n",
    "#     score_th = trial.suggest_float(\"score_th\", 0, 0.5)\n",
    "#     distance = trial.suggest_int(\"distance\", 30, 400)\n",
    "\n",
    "#     pred_df: pl.DataFrame = post_process_for_seg(keys, preds, score_th=score_th, distance=distance)\n",
    "#     score = event_detection_ap(gt_df, pred_df)\n",
    "\n",
    "#     return score\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_sample(gt_df, keys, preds, labels, num_samples=1, num_chunks=10):\n",
    "    # get series ids\n",
    "    series_ids = np.array(list(map(lambda x: x.split(\"_\")[0], keys)))\n",
    "    unique_series_ids = np.unique(series_ids)\n",
    "\n",
    "    # get random series\n",
    "    random_series_ids = np.random.choice(unique_series_ids, num_samples)\n",
    "\n",
    "    for i, random_series_id in enumerate(random_series_ids):\n",
    "        # get random series\n",
    "        series_idx = np.where(series_ids == random_series_id)[0]\n",
    "        this_series_preds = preds[series_idx].reshape(-1, 3)\n",
    "        this_series_labels = labels[series_idx].reshape(-1, 3)\n",
    "\n",
    "        # split series\n",
    "        this_series_preds = np.split(this_series_preds, num_chunks)\n",
    "        this_series_labels = np.split(this_series_labels, num_chunks)\n",
    "        this_series_len = [0] + [len(x) for x in this_series_labels]\n",
    "        this_series_len = np.cumsum(this_series_len)\n",
    "\n",
    "        gt_df = gt_df[gt_df[\"series_id\"] == random_series_id]\n",
    "\n",
    "        fig, axs = plt.subplots(num_chunks, 1, figsize=(20, 5 * num_chunks))\n",
    "\n",
    "        if num_chunks == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for j in range(num_chunks):\n",
    "            this_series_preds_chunk = this_series_preds[j]\n",
    "            this_series_labels_chunk = this_series_labels[j]\n",
    "\n",
    "            # get onset and wakeup idx\n",
    "            gt_tmp = gt_df[\n",
    "                (gt_df[\"step\"] >= this_series_len[j]) & (gt_df[\"step\"] <= this_series_len[j + 1])\n",
    "            ]\n",
    "            onset_idx = gt_tmp.loc[gt_tmp[\"event\"] == \"onset\", \"step\"].to_list()\n",
    "            onset_idx = onset_idx - this_series_len[j]\n",
    "            wakeup_idx = gt_tmp.loc[gt_tmp[\"event\"] == \"wakeup\", \"step\"].to_list()\n",
    "            wakeup_idx = wakeup_idx - this_series_len[j]\n",
    "\n",
    "            axs[j].plot(this_series_preds_chunk[:, 0], label=\"pred_sleep\")\n",
    "            axs[j].plot(this_series_preds_chunk[:, 1], label=\"pred_onset\")\n",
    "            axs[j].plot(this_series_preds_chunk[:, 2], label=\"pred_wakeup\")\n",
    "            axs[j].vlines(onset_idx, 0, 1, label=\"onset\", linestyles=\"dashed\", color=\"C1\")\n",
    "            axs[j].vlines(wakeup_idx, 0, 1, label=\"wakeup\", linestyles=\"dashed\", color=\"C2\")\n",
    "            axs[j].set_ylim(0, 1)\n",
    "            axs[j].set_title(f\"series_id: {random_series_id} chunk_id: {j}\")\n",
    "            axs[j].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_random_sample(gt_df, keys, preds, labels, num_chunks=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "BEST_MODEL = \"ensemble\"\n",
    "FOLD = 0\n",
    "\n",
    "DURATION = 8640\n",
    "DOWNSAMPLE_RATE = 2\n",
    "PHASE = \"train\"\n",
    "EXP_NAME = \"transformer_best_folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[5.0GB(+3.9GB):2.2sec] load test dataloader \n",
      "List of models: ['/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_0.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_2.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_4.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_3.pth']\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_0.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_2.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_4.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_3.pth\n",
      "[5.7GB(+0.7GB):4.8sec] load model \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:38<00:00, 11.80it/s]\n",
      "[7.4GB(+1.7GB):177.8sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:38<00:00, 11.77it/s]\n",
      "[7.4GB(+0.0GB):180.0sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:36<00:00, 11.93it/s]\n",
      "[7.4GB(+0.0GB):176.6sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:36<00:00, 11.89it/s]\n",
      "[7.4GB(+0.0GB):176.8sec] inference \n",
      "[7.4GB(+0.0GB):1.0sec] make submission \n"
     ]
    }
   ],
   "source": [
    "!python -m run.inference\\\n",
    "    dir=local\\\n",
    "    model.params.encoder_name=resnet34\\\n",
    "    model.params.encoder_weights=null\\\n",
    "    num_workers=12\\\n",
    "    exp_name=$EXP_NAME\\\n",
    "    weight.run_name=single\\\n",
    "    batch_size=8\\\n",
    "    duration=$DURATION\\\n",
    "    downsample_rate=$DOWNSAMPLE_RATE\\\n",
    "    pp.score_th=0.0015\\\n",
    "    pp.distance=70\\\n",
    "    phase=$PHASE\\\n",
    "    best_model=$BEST_MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
