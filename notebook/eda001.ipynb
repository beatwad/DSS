{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 3, 5, 6\n",
    "\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# strange_series = [\n",
    "#     \"05e1944c3818\",  # 1\n",
    "#     \"13b4d6a01d27\",  # 4\n",
    "#     \"7476c0bd18d2\",  # 3\n",
    "#     \"5aad18e7ce64\",  # 2\n",
    "#     \"aed3850f65f0\",  # 5\n",
    "#     \"c5365a55ebb7\",  # 2\n",
    "#     \"f981a0805fd0\",  # 4\n",
    "#     \"0f9e60a8e56d\",  # 3\n",
    "#     \"2fc653ca75c7\",  # 5\n",
    "#     \"390b487231ce\",  # 4\n",
    "#     \"89c7daa72eee\",  # 1\n",
    "#     \"a3e59c2ce3f6\",  # 5\n",
    "#     \"c5d08fc3e040\",  # 3\n",
    "#     \"c7b1283bb7eb\",  # 2\n",
    "#     \"e11b9d69f856\",  # 1\n",
    "# ]\n",
    "\n",
    "# x = pd.read_csv(\"../data/child-mind-institute-detect-sleep-states/train_events.csv\")\n",
    "# x[\"is_strange\"] = 0\n",
    "# x[\"fold\"] = 0\n",
    "# x.loc[x[\"series_id\"].isin(strange_series), \"is_strange\"] = 1\n",
    "\n",
    "# sgkf = StratifiedGroupKFold(n_splits=7)\n",
    "\n",
    "# x = x[x[\"event\"] == \"onset\"].reset_index(drop=True)\n",
    "# X = x[[\"night\"]]\n",
    "# y = x[\"is_strange\"]\n",
    "# groups = x[\"series_id\"]\n",
    "\n",
    "# res = \"\"\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(sgkf.split(X, y, groups)):\n",
    "#     series = x.loc[test_index, \"series_id\"].unique()\n",
    "#     x.loc[test_index, \"fold\"] = i\n",
    "#     print(x.loc[test_index, \"step\"].isna().sum())\n",
    "\n",
    "#     for s in series:\n",
    "#         res += f\"- {s}\\n\"\n",
    "#     res += \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "- fix DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632110/3043315833.py:41: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=config_path.as_posix())\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from src.utils.metrics import event_detection_ap\n",
    "\n",
    "from src.utils.post_process import post_process_for_seg\n",
    "import jupyter_black\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "\n",
    "jupyter_black.load()\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "INFERENCE = False\n",
    "EXP_NAME = \"transformer_folds_7_lstm\"\n",
    "RUN_NAME = \"run0\"\n",
    "TYPE = \"score\"\n",
    "\n",
    "if INFERENCE:\n",
    "    RESULT_DIR = Path(\"../output/inference\") / EXP_NAME  # / RUN_NAME\n",
    "    hydra_result_dir = Path(\"../output/train\") / \"transformer_folds_5\" / RUN_NAME\n",
    "else:\n",
    "    RESULT_DIR = Path(\"../output/train\") / EXP_NAME\n",
    "    hydra_result_dir = Path(\"../output/train\") / EXP_NAME / RUN_NAME\n",
    "\n",
    "\n",
    "def load_config(result_dir: Path):\n",
    "    # clear previous initialization\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "    # initialize hydra\n",
    "    config_path = result_dir / \".hydra\"\n",
    "    initialize(config_path=config_path.as_posix())\n",
    "    # load the config\n",
    "    cfg = compose(config_name=\"config\")\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg = load_config(hydra_result_dir)\n",
    "\n",
    "if INFERENCE:\n",
    "    preds = np.load(RESULT_DIR / f\"preds.npy\")\n",
    "    keys = np.load(RESULT_DIR / f\"keys.npy\")\n",
    "else:\n",
    "    preds, labels, keys = None, None, None\n",
    "    for i in range(7):\n",
    "        tmp_preds = np.load(RESULT_DIR / f\"run{i}\" / f\"preds_{TYPE}.npy\")\n",
    "        tmp_labels = np.load(RESULT_DIR / f\"run{i}\" / f\"labels_{TYPE}.npy\")\n",
    "        tmp_keys = np.load(RESULT_DIR / f\"run{i}\" / \"keys.npy\")\n",
    "\n",
    "        if tmp_preds.shape[2] == 3:\n",
    "            tmp_preds = tmp_preds[:, :, [1, 2]]\n",
    "\n",
    "        if tmp_labels.shape[2] == 3:\n",
    "            tmp_labels = tmp_labels[:, :, [1, 2]]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = tmp_preds\n",
    "            labels = tmp_labels\n",
    "            keys = tmp_keys\n",
    "        else:\n",
    "            preds = np.concatenate((preds, tmp_preds))\n",
    "            labels = np.concatenate((labels, tmp_labels))\n",
    "            keys = np.concatenate((keys, tmp_keys))\n",
    "\n",
    "gt_df = pd.read_csv(Path(cfg.dir.data_dir) / \"train_events.csv\").dropna().reset_index(drop=True)\n",
    "# gt_df = gt_df[gt_df[\"series_id\"].isin(cfg.split.valid_series_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize postprocess parameters using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6707851650592838, 470660)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_weak_events(sub_df, event_threshold):\n",
    "    # delete onset and wakeup events that are too far from the same events with high score\n",
    "    sub_df[\"prev_onset_step\"] = np.nan\n",
    "    sub_df[\"next_onset_step\"] = np.nan\n",
    "    sub_df[\"prev_wakeup_step\"] = np.nan\n",
    "    sub_df[\"next_wakeup_step\"] = np.nan\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask = (sub_df[\"event\"] == \"onset\") & (sub_df[\"score\"] >= 0.1)\n",
    "    sub_df.loc[mask, \"prev_high_score_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_high_score_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_high_score_onset_step\"] = sub_df[\"prev_high_score_onset_step\"].ffill()\n",
    "    sub_df[\"next_high_score_onset_step\"] = sub_df[\"next_high_score_onset_step\"].bfill()\n",
    "\n",
    "    mask = (\n",
    "        (sub_df[\"event\"] == \"onset\")\n",
    "        & (sub_df[\"step\"] - sub_df[\"prev_high_score_onset_step\"] >= event_threshold)\n",
    "        & (sub_df[\"next_high_score_onset_step\"] - sub_df[\"step\"] >= event_threshold)\n",
    "    )\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask = (sub_df[\"event\"] == \"wakeup\") & (sub_df[\"score\"] >= 0.1)\n",
    "    sub_df.loc[mask, \"prev_high_score_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_high_score_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_high_score_wakeup_step\"] = sub_df[\"prev_high_score_wakeup_step\"].ffill()\n",
    "    sub_df[\"next_high_score_wakeup_step\"] = sub_df[\"next_high_score_wakeup_step\"].bfill()\n",
    "\n",
    "    mask = (\n",
    "        (sub_df[\"event\"] == \"wakeup\")\n",
    "        & (sub_df[\"step\"] - sub_df[\"prev_high_score_wakeup_step\"] >= event_threshold)\n",
    "        & (sub_df[\"next_high_score_wakeup_step\"] - sub_df[\"step\"] >= event_threshold)\n",
    "    )\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def clean_too_far_events(sub_df, event_threshold):\n",
    "    # delete onset and wakeup events that are too far from next wakeup / previous onset event\n",
    "    sub_df[\"prev_onset_step\"] = np.nan\n",
    "    sub_df[\"next_onset_step\"] = np.nan\n",
    "    sub_df[\"prev_wakeup_step\"] = np.nan\n",
    "    sub_df[\"next_wakeup_step\"] = np.nan\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask = sub_df[\"event\"] == \"onset\"\n",
    "    sub_df.loc[mask, \"prev_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_onset_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_onset_step\"] = sub_df[\"prev_onset_step\"].ffill()\n",
    "    sub_df[\"next_onset_step\"] = sub_df[\"next_onset_step\"].bfill()\n",
    "\n",
    "    mask = sub_df[\"step\"] - sub_df[\"prev_onset_step\"] >= event_threshold\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask = sub_df[\"event\"] == \"wakeup\"\n",
    "    sub_df.loc[mask, \"prev_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df.loc[mask, \"next_wakeup_step\"] = sub_df.loc[mask, \"step\"]\n",
    "    sub_df[\"prev_wakeup_step\"] = sub_df[\"prev_wakeup_step\"].ffill()\n",
    "    sub_df[\"next_wakeup_step\"] = sub_df[\"next_wakeup_step\"].bfill()\n",
    "\n",
    "    mask = sub_df[\"next_wakeup_step\"] - sub_df[\"step\"] >= event_threshold\n",
    "    sub_df.loc[mask, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def delete_alone_events(sub_df, alone_threshold):\n",
    "    # detect alone onset event that are between two other onset events and delete them\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask_1 = (\n",
    "        sub_df[\"step\"] - sub_df[\"prev_onset_step\"].shift(1)\n",
    "        < sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"]\n",
    "    )\n",
    "    mask_2 = sub_df[\"step\"] - sub_df[\"prev_onset_step\"].shift(1) >= alone_threshold\n",
    "    mask_3 = (\n",
    "        sub_df[\"next_onset_step\"].shift(-1) - sub_df[\"step\"]\n",
    "        < sub_df[\"next_wakeup_step\"] - sub_df[\"step\"]\n",
    "    )\n",
    "    mask_4 = sub_df[\"next_onset_step\"].shift(-1) - sub_df[\"step\"] >= alone_threshold\n",
    "    mask_5 = sub_df[\"event\"] == \"onset\"\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask_1 = (\n",
    "        sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"].shift(1)\n",
    "        < sub_df[\"step\"] - sub_df[\"prev_onset_step\"]\n",
    "    )\n",
    "    mask_2 = sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"].shift(1) >= alone_threshold\n",
    "    mask_3 = (\n",
    "        sub_df[\"next_wakeup_step\"].shift(-1) - sub_df[\"step\"]\n",
    "        < sub_df[\"next_onset_step\"] - sub_df[\"step\"]\n",
    "    )\n",
    "    mask_4 = sub_df[\"next_wakeup_step\"].shift(-1) - sub_df[\"step\"] >= alone_threshold\n",
    "    mask_5 = sub_df[\"event\"] == \"wakeup\"\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def delete_events_among_differnet_evnets(sub_df, close_threshold):\n",
    "    # delete events that are among many different events\n",
    "    sub_df[\"to_del\"] = 0\n",
    "\n",
    "    # onset\n",
    "    mask_1 = (\n",
    "        (sub_df[\"event\"].shift(1) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(2) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(3) == \"wakeup\")\n",
    "    )\n",
    "    mask_2 = (\n",
    "        (sub_df[\"event\"].shift(-1) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(-2) == \"wakeup\")\n",
    "        & (sub_df[\"event\"].shift(-3) == \"wakeup\")\n",
    "    )\n",
    "    mask_3 = sub_df[\"event\"] == \"onset\"\n",
    "    mask_4 = sub_df[\"step\"] - sub_df[\"prev_wakeup_step\"] <= close_threshold\n",
    "    mask_5 = sub_df[\"next_wakeup_step\"] - sub_df[\"step\"] <= close_threshold\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # wakeup\n",
    "    mask_1 = (\n",
    "        (sub_df[\"event\"].shift(1) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(2) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(3) == \"onset\")\n",
    "    )\n",
    "    mask_2 = (\n",
    "        (sub_df[\"event\"].shift(-1) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(-2) == \"onset\")\n",
    "        & (sub_df[\"event\"].shift(-3) == \"onset\")\n",
    "    )\n",
    "    mask_3 = sub_df[\"event\"] == \"wakeup\"\n",
    "    mask_4 = sub_df[\"step\"] - sub_df[\"prev_onset_step\"] <= 360\n",
    "    mask_5 = sub_df[\"next_onset_step\"] - sub_df[\"step\"] <= 360\n",
    "    mask_6 = sub_df[\"score\"] < 0.1\n",
    "    sub_df.loc[mask_1 & mask_2 & mask_3 & mask_4 & mask_5 & mask_6, \"to_del\"] = 1\n",
    "\n",
    "    # remove these events\n",
    "    sub_df = sub_df[sub_df[\"to_del\"] == 0]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def post_process_for_seg(\n",
    "    keys: list[str],\n",
    "    preds: np.ndarray,\n",
    "    series_lens: dict,\n",
    "    score_th: float = 0.01,\n",
    "    distance: int = 5000,\n",
    "    offset: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"make submission dataframe for segmentation task\n",
    "\n",
    "    Args:\n",
    "        keys (list[str]): list of keys. key is \"{series_id}_{chunk_id}\"\n",
    "        preds (np.ndarray): (num_series * num_chunks, duration, 2)\n",
    "        score_th (float, optional): threshold for score. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: submission dataframe\n",
    "    \"\"\"\n",
    "    series_ids = np.array(list(map(lambda x: x.split(\"_\")[0], keys)))\n",
    "    unique_series_ids = np.unique(series_ids)\n",
    "\n",
    "    records = []\n",
    "    for series_id in unique_series_ids:\n",
    "        max_step = series_lens[series_id]\n",
    "\n",
    "        series_idx = np.where(series_ids == series_id)[0]\n",
    "        this_series_preds = preds[series_idx].reshape(-1, 2)\n",
    "\n",
    "        onset_event_preds = this_series_preds[:, 0]\n",
    "        onset_steps = find_peaks(onset_event_preds, height=score_th, distance=distance)[0]\n",
    "        onset_scores = onset_event_preds[onset_steps]\n",
    "        min_onset_step = min(onset_steps) if len(onset_steps) > 0 else 0\n",
    "\n",
    "        wakeup_event_preds = this_series_preds[:, 1]\n",
    "        wakeup_steps = find_peaks(wakeup_event_preds, height=score_th, distance=distance)[0]\n",
    "        wakeup_scores = wakeup_event_preds[wakeup_steps]\n",
    "        max_wakeup_step = max(wakeup_steps) if len(wakeup_steps) > 0 else 0\n",
    "\n",
    "        for step, score in zip(onset_steps, onset_scores):\n",
    "            # select only wakeups than has at least one onset before\n",
    "            # and not too close to series borders\n",
    "            if step >= max_wakeup_step or step <= 720 or step >= max_step - 720 * offset:\n",
    "                continue\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"series_id\": series_id,\n",
    "                    \"step\": step,\n",
    "                    \"event\": \"onset\",\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        for step, score in zip(wakeup_steps, wakeup_scores):\n",
    "            # select only onsets than has at least one wakeup after\n",
    "            # and not too close to series borders\n",
    "            if step <= min_onset_step or step <= 720 * offset or step >= max_step - 180:\n",
    "                continue\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"series_id\": series_id,\n",
    "                    \"step\": step,\n",
    "                    \"event\": \"wakeup\",\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if len(records) == 0:  # If there is no prediction, insert dummy\n",
    "        records.append(\n",
    "            {\n",
    "                \"series_id\": series_id,\n",
    "                \"step\": 0,\n",
    "                \"event\": \"onset\",\n",
    "                \"score\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    sub_df = pd.DataFrame(records).sort_values([\"series_id\", \"step\"]).reset_index(drop=True)\n",
    "    sub_df[\"row_id\"] = np.arange(len(sub_df))\n",
    "\n",
    "    # # delete onset and wakeup events that are too far from the same events with high score\n",
    "    # high_score_event_threshold = 17280 * 1.01  # 24 hours\n",
    "    # sub_df = clean_weak_events(sub_df, high_score_event_threshold)\n",
    "\n",
    "    # # delete onset and wakeup events that are too far from next wakeup / previous onset event\n",
    "    # event_threshold = 14400  # 20 hours\n",
    "    # sub_df = clean_too_far_events(sub_df, event_threshold)\n",
    "\n",
    "    # # detect alone onset event that are between two other onset events and delete them\n",
    "    # alone_threshold = 2880  # 2 hours\n",
    "    # sub_df = delete_alone_events(sub_df, alone_threshold)\n",
    "\n",
    "    # # delete events that are among many different events\n",
    "    # close_threshold = 360  # 30 minutes\n",
    "    # sub_df = delete_events_among_differnet_evnets(sub_df, close_threshold)\n",
    "\n",
    "    sub_df = sub_df[[\"row_id\", \"series_id\", \"step\", \"event\", \"score\"]]\n",
    "\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "with open(Path(cfg.dir.processed_dir) / \"train\" / \"series_lens.json\") as f:\n",
    "    series_lens = json.load(f)\n",
    "\n",
    "pred_df = post_process_for_seg(keys, preds, series_lens, score_th=0.0001, distance=70, offset=8)\n",
    "# pred_df = pred_df.to_pandas()\n",
    "\n",
    "score = event_detection_ap(gt_df, pred_df)\n",
    "\n",
    "score, len(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.7675050274210242, 468368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):\n",
    "#     # score_th = 0.005\n",
    "#     score_th = trial.suggest_float(\"score_th\", 0, 0.5)\n",
    "#     distance = trial.suggest_int(\"distance\", 30, 400)\n",
    "\n",
    "#     pred_df: pl.DataFrame = post_process_for_seg(keys, preds, score_th=score_th, distance=distance)\n",
    "#     score = event_detection_ap(gt_df, pred_df)\n",
    "\n",
    "#     return score\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_sample(gt_df, keys, preds, labels, num_samples=1, num_chunks=10):\n",
    "    # get series ids\n",
    "    series_ids = np.array(list(map(lambda x: x.split(\"_\")[0], keys)))\n",
    "    unique_series_ids = np.unique(series_ids)\n",
    "\n",
    "    # get random series\n",
    "    random_series_ids = np.random.choice(unique_series_ids, num_samples)\n",
    "\n",
    "    for i, random_series_id in enumerate(random_series_ids):\n",
    "        # get random series\n",
    "        series_idx = np.where(series_ids == random_series_id)[0]\n",
    "        this_series_preds = preds[series_idx].reshape(-1, 3)\n",
    "        this_series_labels = labels[series_idx].reshape(-1, 3)\n",
    "\n",
    "        # split series\n",
    "        this_series_preds = np.split(this_series_preds, num_chunks)\n",
    "        this_series_labels = np.split(this_series_labels, num_chunks)\n",
    "        this_series_len = [0] + [len(x) for x in this_series_labels]\n",
    "        this_series_len = np.cumsum(this_series_len)\n",
    "\n",
    "        gt_df = gt_df[gt_df[\"series_id\"] == random_series_id]\n",
    "\n",
    "        fig, axs = plt.subplots(num_chunks, 1, figsize=(20, 5 * num_chunks))\n",
    "\n",
    "        if num_chunks == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for j in range(num_chunks):\n",
    "            this_series_preds_chunk = this_series_preds[j]\n",
    "            this_series_labels_chunk = this_series_labels[j]\n",
    "\n",
    "            # get onset and wakeup idx\n",
    "            gt_tmp = gt_df[\n",
    "                (gt_df[\"step\"] >= this_series_len[j]) & (gt_df[\"step\"] <= this_series_len[j + 1])\n",
    "            ]\n",
    "            onset_idx = gt_tmp.loc[gt_tmp[\"event\"] == \"onset\", \"step\"].to_list()\n",
    "            onset_idx = onset_idx - this_series_len[j]\n",
    "            wakeup_idx = gt_tmp.loc[gt_tmp[\"event\"] == \"wakeup\", \"step\"].to_list()\n",
    "            wakeup_idx = wakeup_idx - this_series_len[j]\n",
    "\n",
    "            axs[j].plot(this_series_preds_chunk[:, 0], label=\"pred_sleep\")\n",
    "            axs[j].plot(this_series_preds_chunk[:, 1], label=\"pred_onset\")\n",
    "            axs[j].plot(this_series_preds_chunk[:, 2], label=\"pred_wakeup\")\n",
    "            axs[j].vlines(onset_idx, 0, 1, label=\"onset\", linestyles=\"dashed\", color=\"C1\")\n",
    "            axs[j].vlines(wakeup_idx, 0, 1, label=\"wakeup\", linestyles=\"dashed\", color=\"C2\")\n",
    "            axs[j].set_ylim(0, 1)\n",
    "            axs[j].set_title(f\"series_id: {random_series_id} chunk_id: {j}\")\n",
    "            axs[j].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_random_sample(gt_df, keys, preds, labels, num_chunks=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "BEST_MODEL = \"ensemble\"\n",
    "FOLD = 0\n",
    "\n",
    "DURATION = 8640\n",
    "DOWNSAMPLE_RATE = 2\n",
    "PHASE = \"train\"\n",
    "EXP_NAME = \"transformer_best_folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[5.0GB(+3.9GB):2.2sec] load test dataloader \n",
      "List of models: ['/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_0.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_2.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_4.pth', '/home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_3.pth']\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_0.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_2.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_10_4.pth\n",
      "load weight from /home/alex/Kaggle/DSS/output/train/transformer_best_folds/fold_3.pth\n",
      "[5.7GB(+0.7GB):4.8sec] load model \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:38<00:00, 11.80it/s]\n",
      "[7.4GB(+1.7GB):177.8sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:38<00:00, 11.77it/s]\n",
      "[7.4GB(+0.0GB):180.0sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:36<00:00, 11.93it/s]\n",
      "[7.4GB(+0.0GB):176.6sec] inference \n",
      "inference: 100%|████████████████████████████| 1867/1867 [02:36<00:00, 11.89it/s]\n",
      "[7.4GB(+0.0GB):176.8sec] inference \n",
      "[7.4GB(+0.0GB):1.0sec] make submission \n"
     ]
    }
   ],
   "source": [
    "!python -m run.inference\\\n",
    "    dir=local\\\n",
    "    model.params.encoder_name=resnet34\\\n",
    "    model.params.encoder_weights=null\\\n",
    "    num_workers=12\\\n",
    "    exp_name=$EXP_NAME\\\n",
    "    weight.run_name=single\\\n",
    "    batch_size=8\\\n",
    "    duration=$DURATION\\\n",
    "    downsample_rate=$DOWNSAMPLE_RATE\\\n",
    "    pp.score_th=0.0015\\\n",
    "    pp.distance=70\\\n",
    "    phase=$PHASE\\\n",
    "    best_model=$BEST_MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
